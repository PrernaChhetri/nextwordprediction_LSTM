{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3204,
     "status": "ok",
     "timestamp": 1697215418881,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "rker6hNl3IZV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 195509,
     "status": "ok",
     "timestamp": 1697215620889,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "PJVNiIPO9yxd",
    "outputId": "3ac58e78-3db0-4495-e46c-a0a448ed0ad7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-e71b13f5-f694-4a30-bd71-8005d5b22f94\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-e71b13f5-f694-4a30-bd71-8005d5b22f94\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sherlock-holm.es_stories_plain-text_advs.txt to sherlock-holm.es_stories_plain-text_advs.txt\n"
     ]
    }
   ],
   "source": [
    "#Need to upload the file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33446,
     "status": "ok",
     "timestamp": 1697215682793,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "R2PuqOuj5275",
    "outputId": "b463d9c4-7d53-4e72-bc5e-f60559c9f194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/FYP\n"
     ]
    }
   ],
   "source": [
    "#Mount Drive\n",
    "from google.colab import drive as mountGoogleDrive\n",
    "mountGoogleDrive.mount('/content/FYP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1697215710486,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "8yLL6BI05Uf6"
   },
   "outputs": [],
   "source": [
    "#Read the text file\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt' , 'r', encoding='utf-8') as file:\n",
    "  text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beH0srxo-x6C"
   },
   "source": [
    "##The following method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1697215721087,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "s66rBRCR7SuK"
   },
   "outputs": [],
   "source": [
    "#Now the Tokinization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1697215724968,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "UQgMw21u_HGn"
   },
   "outputs": [],
   "source": [
    "#Now forming the N-grams from the sequences\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1697216249563,
     "user": {
      "displayName": "TENZIN YOEZER",
      "userId": "14539219024243838893"
     },
     "user_tz": -360
    },
    "id": "T2FSuTSjALc9"
   },
   "outputs": [],
   "source": [
    "#input sequences are padded to have same length\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XvQI3BvA4w6"
   },
   "outputs": [],
   "source": [
    "#split into input and output\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx5P4XJHBV7J"
   },
   "outputs": [],
   "source": [
    "#output is converted to one-hot encode vectors\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3255,
     "status": "ok",
     "timestamp": 1696752900007,
     "user": {
      "displayName": "PRERNA CHHETRI",
      "userId": "16006820956955369444"
     },
     "user_tz": -360
    },
    "id": "X1QudjL3Bvdg",
    "outputId": "b847a72c-4ae1-4459-b1b7-d2a445cd8cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2208800 (8.43 MB)\n",
      "Trainable params: 2208800 (8.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#building a neural network architecture to train the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2250817,
     "status": "ok",
     "timestamp": 1696755152352,
     "user": {
      "displayName": "PRERNA CHHETRI",
      "userId": "16006820956955369444"
     },
     "user_tz": -360
    },
    "id": "4owT1g3xCsnJ",
    "outputId": "e6e7c124-6112-4add-a0c2-2e8f58448a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3010/3010 [==============================] - 47s 13ms/step - loss: 6.2463 - accuracy: 0.0757\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 5.5162 - accuracy: 0.1244\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 5.1318 - accuracy: 0.1479\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 4.8102 - accuracy: 0.1657\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 4.5133 - accuracy: 0.1823\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 4.2287 - accuracy: 0.2027\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 3.9576 - accuracy: 0.2248\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 3.7019 - accuracy: 0.2548\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 3.4610 - accuracy: 0.2875\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 3.2362 - accuracy: 0.3213\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 3.0297 - accuracy: 0.3561\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 2.8356 - accuracy: 0.3881\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 2.6590 - accuracy: 0.4211\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 23s 7ms/step - loss: 2.4968 - accuracy: 0.4519\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 2.3433 - accuracy: 0.4831\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 2.2050 - accuracy: 0.5110\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 2.0780 - accuracy: 0.5362\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.9582 - accuracy: 0.5624\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.8507 - accuracy: 0.5861\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.7498 - accuracy: 0.6079\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.6576 - accuracy: 0.6272\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.5729 - accuracy: 0.6465\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 1.4957 - accuracy: 0.6626\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.4221 - accuracy: 0.6800\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.3557 - accuracy: 0.6930\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.2941 - accuracy: 0.7073\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.2393 - accuracy: 0.7177\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.1856 - accuracy: 0.7315\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.1396 - accuracy: 0.7413\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.0942 - accuracy: 0.7515\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 1.0540 - accuracy: 0.7593\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 1.0148 - accuracy: 0.7681\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.9798 - accuracy: 0.7754\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.9477 - accuracy: 0.7825\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.9163 - accuracy: 0.7891\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 0.8918 - accuracy: 0.7953\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.8646 - accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.8417 - accuracy: 0.8055\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 0.8179 - accuracy: 0.8095\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.7988 - accuracy: 0.8150\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.7806 - accuracy: 0.8190\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.7647 - accuracy: 0.8205\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.7455 - accuracy: 0.8260\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 0.7325 - accuracy: 0.8282\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.7170 - accuracy: 0.8323\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.7037 - accuracy: 0.8342\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 0.6902 - accuracy: 0.8371\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6788 - accuracy: 0.8398\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6686 - accuracy: 0.8414\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6638 - accuracy: 0.8413\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6524 - accuracy: 0.8434\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6436 - accuracy: 0.8459\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6363 - accuracy: 0.8461\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6298 - accuracy: 0.8490\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6238 - accuracy: 0.8485\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6152 - accuracy: 0.8514\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.6076 - accuracy: 0.8527\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 21s 7ms/step - loss: 0.6077 - accuracy: 0.8513\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 23s 7ms/step - loss: 0.5963 - accuracy: 0.8549\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5928 - accuracy: 0.8547\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5854 - accuracy: 0.8568\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5862 - accuracy: 0.8559\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5812 - accuracy: 0.8570\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5770 - accuracy: 0.8575\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5711 - accuracy: 0.8593\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5647 - accuracy: 0.8588\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5635 - accuracy: 0.8588\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5668 - accuracy: 0.8580\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5556 - accuracy: 0.8611\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5592 - accuracy: 0.8601\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 23s 7ms/step - loss: 0.5558 - accuracy: 0.8603\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5465 - accuracy: 0.8623\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5520 - accuracy: 0.8613\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5474 - accuracy: 0.8609\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5447 - accuracy: 0.8619\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5452 - accuracy: 0.8612\n",
      "Epoch 77/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5351 - accuracy: 0.8643\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5395 - accuracy: 0.8626\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5344 - accuracy: 0.8632\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5340 - accuracy: 0.8633\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5349 - accuracy: 0.8635\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5332 - accuracy: 0.8633\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5326 - accuracy: 0.8634\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5315 - accuracy: 0.8638\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5284 - accuracy: 0.8630\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5224 - accuracy: 0.8659\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5257 - accuracy: 0.8643\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5262 - accuracy: 0.8642\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5194 - accuracy: 0.8664\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5185 - accuracy: 0.8666\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5260 - accuracy: 0.8623\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5200 - accuracy: 0.8647\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5331 - accuracy: 0.8604\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5169 - accuracy: 0.8648\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5197 - accuracy: 0.8646\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5166 - accuracy: 0.8649\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5192 - accuracy: 0.8633\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5167 - accuracy: 0.8643\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 22s 7ms/step - loss: 0.5142 - accuracy: 0.8644\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 23s 8ms/step - loss: 0.5184 - accuracy: 0.8637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x79e08dc92f50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "model.compile(loss='categorical_crossentropy' , optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1696755820324,
     "user": {
      "displayName": "PRERNA CHHETRI",
      "userId": "16006820956955369444"
     },
     "user_tz": -360
    },
    "id": "mT_mHJV2AwGg",
    "outputId": "77a918b2-a40d-48ab-80f2-ff69b24c660b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "I will leave if they were a little\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I will leave if they\"\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1696755828076,
     "user": {
      "displayName": "PRERNA CHHETRI",
      "userId": "16006820956955369444"
     },
     "user_tz": -360
    },
    "id": "3PTZGDcHBTHR",
    "outputId": "83317ef4-2edf-496a-d2dc-889d6434be64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1693904759235,
     "user": {
      "displayName": "PRERNA CHHETRI",
      "userId": "16006820956955369444"
     },
     "user_tz": -360
    },
    "id": "YN9VbPYLNTTg",
    "outputId": "1e24854a-ae46-46a1-f734-26b4f819a6e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "I ate is john george\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"I love\"\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
